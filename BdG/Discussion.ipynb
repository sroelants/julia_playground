{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "push!(LOAD_PATH, \"./\")\n",
    "using BdgSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workspace()\n",
    "reload(\"BdgSolver\")\n",
    "using BdgSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sh = Shape(10, 10, 10)\n",
    "mat = Material(\"Sn\", 1.0, 1.0, 1.0)\n",
    "pars = Parameters(mat, sh)\n",
    "sys = System(mat, sh, pars)\n",
    "H = Hamiltonian(mat, sh, pars);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Clearly, integrating like this won't be compatible with being able to apply corrections as convolutions to the thermal weight or DOS. I simply can't do \"symbolic\" integrations for all the corrections and get a function as end result. I'll have to discretize at least part of it, on which I can apply the corrections. I think it's cleanest if I just apply *all* corrections to the DOS:\n",
    "\n",
    "$$M_{ij} = \\Phi_{ij} \\int_{DW} d\\xi\\, N_i(\\xi) \\int d\\Xi\\, F(\\xi - \\Xi, \\Gamma_\\xi) \\frac{ \\textrm{tanh} \\frac{\\beta_c \\Xi}{2}}{\\Xi}$$\n",
    "\n",
    "If we put in the Debye-window cutoff manually as a characteristic function, $\\chi_{DW}(\\xi)$, both integrations become completely equivalent (at least in principle).\n",
    "\n",
    "$$M_{ij} = \\Phi_{ij} \\int d\\xi\\, \\chi_{DW}(\\xi)N_i(\\xi) \\int d\\Xi\\, F(\\xi - \\Xi, \\Gamma_\\xi) \\frac{ \\textrm{tanh} \\frac{\\beta_c \\Xi}{2}}{\\Xi}.$$\n",
    "\n",
    "Reversing the integrations:\n",
    "$$M_{ij} = \\Phi_{ij} \\int d\\Xi\\, \\frac{ \\textrm{tanh} \\frac{\\beta_c \\Xi}{2}}{\\Xi}  \\left\\{\\int d\\xi\\, F(\\xi - \\Xi, \\Gamma_\\xi) \\chi_{DW}(\\xi)N_i(\\xi)\\right\\}.$$\n",
    "\n",
    "This way, I can repeatedly keep applying corrections to the DOS. I think. Right?\n",
    "\n",
    "\n",
    "Note that, if I choose to discretize the DOS, and apply all corrections at the DOS level, I will essentially be multiplying the number of operations (and memory usage) by a factor $\\nu$... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thermal determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thought of something!\n",
    "\n",
    "The determinant I have to compute essentialy comes down to $\\det\\left(\\Phi_{ij} I_i\\right)$, where $\\Phi_{ij}$ are the overlap elements, and $I_i$ is the thermal integral.\n",
    "\n",
    "We can easily write this down as a proper matrix product: $\\Phi_{ij} (\\delta_{jk}I_k)$. That is to say, the same can be achieved by multiplying $\\Phi$ with a matrix $I$ having the $I_i$ on the diagonal. The determinant of this matrix product factorizes, of course, $\\det(\\Phi I) = \\det\\Phi \\det I$, and we are left to compute the determinant of $\\Phi$ (trivial), and the determinant of the diagonal matrix $I$, which is cleary very easy.\n",
    "\n",
    "I wonder if there's a difference in computation time of calculating the determinant simply as the product, or running the `det` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13.470774 seconds (7 allocations: 762.940 MB, 0.49% gc time)\n",
      "  0.000024 seconds (5 allocations: 176 bytes)\n",
      "  0.148644 seconds (8 allocations: 78.375 KB)\n"
     ]
    }
   ],
   "source": [
    "x = rand(10000) + 0.6\n",
    "@time X = diagm(x)\n",
    "@time prod(x)\n",
    "@time det(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah yes, not only is the determinant a sot more work (by a factor 1000 or so), just STORING the diagonal matrix is ridiculous. Of course, I could simply store it as a sparse matrix, but then I would get the overhead of going to and from a sparse form, right?\n",
    "\n",
    "This could make stuff a lot more efficient!\n",
    "\n",
    "Okay, granted, $\\Phi$ is only a $\\nu\\times\\nu$ matrix, so it's not terribly expensive, but still, this should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¾s shape: (50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.043199007720004505"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: (50,)\n",
      "N shape: (50,11)\n",
      "I shape: (1,11)\n"
     ]
    }
   ],
   "source": [
    "BdgSolver.get_Tc(sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next challenge: The determinant will, because of the windowing, inherently contain a whole bunch of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.3",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
